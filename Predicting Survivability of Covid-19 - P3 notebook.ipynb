{"cells": [{"metadata": {"collapsed": true}, "cell_type": "markdown", "source": "################################################################################\n#Licensed Materials - Property of IBM\n#(C) Copyright IBM Corp. 2019\n#US Government Users Restricted Rights - Use, duplication disclosure restricted\n#by GSA ADP Schedule Contract with IBM Corp.\n################################################################################\n\nThe auto-generated notebooks are subject to the International License Agreement for Non-Warranted Programs (or equivalent) and License Information document for Watson Studio Auto-generated Notebook (\"License Terms\"), such agreements located in the link below.\nSpecifically, the Source Components and Sample Materials clause included in the License Information document for\nWatson Studio Auto-generated Notebook applies to the auto-generated notebooks. \nBy downloading, copying, accessing, or otherwise using the materials, you agree to the License Terms.\nhttp://www14.software.ibm.com/cgi-bin/weblap/lap.pl?li_formnum=L-AMCU-BHU2B7&title=IBM%20Watson%20Studio%20Auto-generated%20Notebook%20V2.1\n"}, {"metadata": {}, "cell_type": "markdown", "source": "## IBM AutoAI Auto-Generated Notebook v1.11.7\n### Representing Pipeline: P3 from run 5c0006c0-3c45-4f8d-a5ea-9162c8890cd0\n\n**Note**: Notebook code generated using AutoAI will execute successfully.\nIf code is modified or reordered, there is no guarantee it will successfully execute.\nThis pipeline is optimized for the original dataset.  The pipeline may fail or produce sub-optimium results if used with different data.\nFor different data, please consider returning to AutoAI Experiements to generate a new pipeline.\nPlease read our documentation for more information:  \n(IBM Cloud Platform) https://dataplatform.cloud.ibm.com/docs/content/wsj/analyze-data/autoai-notebook.html .  \n(IBM Cloud Pak For Data) https://www.ibm.com/support/knowledgecenter/SSQNUZ_3.0.0/wsj/analyze-data/autoai-notebook.html .  \n\nBefore modifying the pipeline or trying to re-fit the pipeline, consider:  \nThe notebook converts dataframes to numpy arrays before fitting the pipeline (a current restriction of the preprocessor pipeline).\nThe known_values_list is passed by reference and populated with categorical values during fit of the preprocessing pipeline.  Delete its members before re-fitting.\n"}, {"metadata": {}, "cell_type": "markdown", "source": "### 1. Set Up"}, {"metadata": {}, "cell_type": "code", "source": "#attempt import of autoai_libs and install if missing\ntry:\n    import autoai_libs\nexcept Exception as e:\n    print('attempting to install missing autoai_libs from pypi, this may take tens of seconds to complete.')\n    import subprocess\n    try:\n        # attempt to install missing autoai-libs from pypi\n        out = subprocess.check_output('pip install autoai-libs', shell=True)\n        for line in out.splitlines():\n            print(line)\n    except Exception as e:\n        print(str(e))\ntry:\n    import autoai_libs\nexcept Exception as e:\n    print('attempting to install missing autoai_libs from local filesystem, this may take tens of seconds to complete.')\n    import subprocess\n    # attempt to install missing autoai-libs from local filesystem\n    try:\n        out = subprocess.check_output('pip install .', shell=True, cwd='software/autoai_libs')\n        for line in out.splitlines():\n            print(line)\n        import autoai_libs\n    except Exception as e:\n        print(str(e))\nimport sklearn\ntry:\n    import xgboost\nexcept:\n    print('xgboost, if needed, will be installed and imported later')\ntry:\n    import lightgbm\nexcept:\n    print('lightgbm, if needed, will be installed and imported later')\nfrom sklearn.cluster import FeatureAgglomeration\nimport numpy\nfrom numpy import inf, nan, dtype, mean\nfrom autoai_libs.sklearn.custom_scorers import CustomScorers\nfrom autoai_libs.cognito.transforms.transform_utils import TExtras, FC\nfrom autoai_libs.transformers.exportable import *\nfrom autoai_libs.utils.exportable_utils import *\nfrom sklearn.pipeline import Pipeline\nknown_values_list=[]\n", "execution_count": 1, "outputs": [{"output_type": "stream", "text": "lightgbm, if needed, will be installed and imported later\n", "name": "stdout"}]}, {"metadata": {}, "cell_type": "code", "source": "# compose a decorator to assist pipeline instantiation via import of modules and installation of packages\ndef decorator_retries(func):\n    def install_import_retry(*args, **kwargs):\n        retries = 0\n        successful = False\n        failed_retries = 0\n        while retries < 100 and failed_retries < 10 and not successful:\n            retries += 1\n            failed_retries += 1\n            try:\n                result = func(*args, **kwargs)\n                successful = True\n            except Exception as e:\n                estr = str(e)\n                if estr.startswith('name ') and estr.endswith(' is not defined'):\n                    try:\n                        import importlib\n                        module_name = estr.split(\"'\")[1]\n                        module = importlib.import_module(module_name)\n                        globals().update({module_name: module})\n                        print('import successful for ' + module_name)\n                        failed_retries -= 1\n                    except Exception as import_failure:\n                        print('import of ' + module_name + ' failed with: ' + str(import_failure))\n                        import subprocess\n                        print('attempting pip install of ' + module_name)\n                        process = subprocess.Popen('pip install ' + module_name, shell=True)\n                        process.wait()\n                        try:\n                            print('re-attempting import of ' + module_name)\n                            module = importlib.import_module(module_name)\n                            globals().update({module_name: module})\n                            print('import successful for ' + module_name)\n                            failed_retries -= 1\n                        except Exception as import_or_installation_failure:\n                            print('failure installing and/or importing ' + module_name + ' error was: ' + str(\n                                import_or_installation_failure))\n                            raise (ModuleNotFoundError('Missing package in environment for ' + module_name +\n                                                       '? Try import and/or pip install manually?'))\n                elif type(e) is AttributeError:\n                    if 'module ' in estr and ' has no attribute ' in estr:\n                        pieces = estr.split(\"'\")\n                        if len(pieces) == 5:\n                            try:\n                                import importlib\n                                print('re-attempting import of ' + pieces[3] + ' from ' + pieces[1])\n                                module = importlib.import_module('.' + pieces[3], pieces[1])\n                                failed_retries -= 1\n                            except:\n                                print('failed attempt to import ' + pieces[3])\n                                raise (e)\n                        else:\n                            raise (e)\n                else:\n                    raise (e)\n        if successful:\n            print('Pipeline successfully instantiated')\n        else:\n            raise (ModuleNotFoundError(\n                'Remaining missing imports/packages in environment? Retry cell and/or try pip install manually?'))\n        return result\n    return install_import_retry\n", "execution_count": 2, "outputs": []}, {"metadata": {}, "cell_type": "markdown", "source": "### 2. Compose Pipeline"}, {"metadata": {}, "cell_type": "code", "source": "# metadata necessary to replicate AutoAI scores with the pipeline\n_input_metadata = {'run_uid': '5c0006c0-3c45-4f8d-a5ea-9162c8890cd0', 'pn': 'P3', 'data_source': '', 'target_label_name': 'Survival_Status', 'learning_type': 'classification', 'optimization_metric': 'accuracy', 'random_state': 33, 'cv_num_folds': 3, 'holdout_fraction': 0.1, 'pos_label': 'Yes'}\n\n# define a function to compose the pipeline, and invoke it\n@decorator_retries\ndef compose_pipeline():\n    import numpy\n    from numpy import nan, dtype, mean\n    #\n    # composing steps for toplevel Pipeline\n    #\n    _input_metadata = {'run_uid': '5c0006c0-3c45-4f8d-a5ea-9162c8890cd0', 'pn': 'P3', 'data_source': '', 'target_label_name': 'Survival_Status', 'learning_type': 'classification', 'optimization_metric': 'accuracy', 'random_state': 33, 'cv_num_folds': 3, 'holdout_fraction': 0.1, 'pos_label': 'Yes'}\n    steps = []\n    #\n    # composing steps for preprocessor Pipeline\n    #\n    preprocessor__input_metadata = None\n    preprocessor_steps = []\n    #\n    # composing steps for preprocessor_features FeatureUnion\n    #\n    preprocessor_features_transformer_list = []\n    #\n    # composing steps for preprocessor_features_categorical Pipeline\n    #\n    preprocessor_features_categorical__input_metadata = None\n    preprocessor_features_categorical_steps = []\n    preprocessor_features_categorical_steps.append(('cat_column_selector', autoai_libs.transformers.exportable.NumpyColumnSelector(columns=[1, 2, 3, 4, 5, 6, 7])))\n    preprocessor_features_categorical_steps.append(('cat_compress_strings', autoai_libs.transformers.exportable.CompressStrings(activate_flag=True, compress_type='hash', dtypes_list=['char_str', 'char_str', 'char_str', 'char_str', 'char_str', 'char_str', 'char_str'], missing_values_reference_list=['', '-', '?', nan], misslist_list=[[], [], [], [], [], [], []])))\n    preprocessor_features_categorical_steps.append(('cat_missing_replacer', autoai_libs.transformers.exportable.NumpyReplaceMissingValues(filling_values=nan, missing_values=[])))\n    preprocessor_features_categorical_steps.append(('cat_unknown_replacer', autoai_libs.transformers.exportable.NumpyReplaceUnknownValues(filling_values=nan, filling_values_list=[nan, nan, nan, nan, nan, nan, nan], known_values_list=known_values_list, missing_values_reference_list=['', '-', '?', nan])))\n    preprocessor_features_categorical_steps.append(('boolean2float_transformer', autoai_libs.transformers.exportable.boolean2float(activate_flag=True)))\n    preprocessor_features_categorical_steps.append(('cat_imputer', autoai_libs.transformers.exportable.CatImputer(activate_flag=True, missing_values=nan, sklearn_version_family='20', strategy='most_frequent')))\n    preprocessor_features_categorical_steps.append(('cat_encoder', autoai_libs.transformers.exportable.CatEncoder(activate_flag=True, categories='auto', dtype=numpy.float64, encoding='ordinal', handle_unknown='error', sklearn_version_family='20')))\n    preprocessor_features_categorical_steps.append(('float32_transformer', autoai_libs.transformers.exportable.float32_transform(activate_flag=True)))\n    # assembling preprocessor_features_categorical_ Pipeline\n    preprocessor_features_categorical_pipeline = sklearn.pipeline.Pipeline(steps=preprocessor_features_categorical_steps)\n    preprocessor_features_transformer_list.append(('categorical', preprocessor_features_categorical_pipeline))\n    #\n    # composing steps for preprocessor_features_numeric Pipeline\n    #\n    preprocessor_features_numeric__input_metadata = None\n    preprocessor_features_numeric_steps = []\n    preprocessor_features_numeric_steps.append(('num_column_selector', autoai_libs.transformers.exportable.NumpyColumnSelector(columns=[0])))\n    preprocessor_features_numeric_steps.append(('num_floatstr2float_transformer', autoai_libs.transformers.exportable.FloatStr2Float(activate_flag=True, dtypes_list=['int_num'], missing_values_reference_list=[])))\n    preprocessor_features_numeric_steps.append(('num_missing_replacer', autoai_libs.transformers.exportable.NumpyReplaceMissingValues(filling_values=nan, missing_values=[])))\n    preprocessor_features_numeric_steps.append(('num_imputer', autoai_libs.transformers.exportable.NumImputer(activate_flag=True, missing_values=nan, strategy='median')))\n    preprocessor_features_numeric_steps.append(('num_scaler', autoai_libs.transformers.exportable.OptStandardScaler(num_scaler_copy=None, num_scaler_with_mean=None, num_scaler_with_std=None, use_scaler_flag=False)))\n    preprocessor_features_numeric_steps.append(('float32_transformer', autoai_libs.transformers.exportable.float32_transform(activate_flag=True)))\n    # assembling preprocessor_features_numeric_ Pipeline\n    preprocessor_features_numeric_pipeline = sklearn.pipeline.Pipeline(steps=preprocessor_features_numeric_steps)\n    preprocessor_features_transformer_list.append(('numeric', preprocessor_features_numeric_pipeline))\n    # assembling preprocessor_features_ FeatureUnion\n    preprocessor_features_pipeline = sklearn.pipeline.FeatureUnion(transformer_list=preprocessor_features_transformer_list)\n    preprocessor_steps.append(('features', preprocessor_features_pipeline))\n    preprocessor_steps.append(('permuter', autoai_libs.transformers.exportable.NumpyPermuteArray(axis=0, permutation_indices=[1, 2, 3, 4, 5, 6, 7, 0])))\n    # assembling preprocessor_ Pipeline\n    preprocessor_pipeline = sklearn.pipeline.Pipeline(steps=preprocessor_steps)\n    steps.append(('preprocessor', preprocessor_pipeline))\n    #\n    # composing steps for cognito Pipeline\n    #\n    cognito__input_metadata = None\n    cognito_steps = []\n    cognito_steps.append(('0', autoai_libs.cognito.transforms.transform_utils.TA1(fun=numpy.square, name='square', datatypes=['numeric'], feat_constraints=[autoai_libs.utils.fc_methods.is_not_categorical], tgraph=None, apply_all=True, col_names=['age', 'marital_status', 'race', 'gender', 'occupation', 'education_level', 'overseas_travel_history', 'other_health_conditions'], col_dtypes=[dtype('float32'), dtype('float32'), dtype('float32'), dtype('float32'), dtype('float32'), dtype('float32'), dtype('float32'), dtype('float32')], col_as_json_objects=None)))\n    cognito_steps.append(('1', autoai_libs.cognito.transforms.transform_utils.FS1(cols_ids_must_keep=range(0, 8), additional_col_count_to_keep=8, ptype='classification')))\n    # assembling cognito_ Pipeline\n    cognito_pipeline = sklearn.pipeline.Pipeline(steps=cognito_steps)\n    steps.append(('cognito', cognito_pipeline))\n    steps.append(('estimator', sklearn.ensemble.gradient_boosting.GradientBoostingClassifier(criterion='friedman_mse', init=None, learning_rate=0.1, loss='deviance', max_depth=3, max_features=None, max_leaf_nodes=None, min_impurity_decrease=0.0, min_impurity_split=None, min_samples_leaf=1, min_samples_split=2, min_weight_fraction_leaf=0.0, n_estimators=100, n_iter_no_change=None, presort='auto', random_state=33, subsample=1.0, tol=0.0001, validation_fraction=0.1, verbose=0, warm_start=False)))\n    # assembling  Pipeline\n    pipeline = sklearn.pipeline.Pipeline(steps=steps)\n    return pipeline\npipeline = compose_pipeline()\n", "execution_count": 3, "outputs": [{"output_type": "stream", "text": "re-attempting import of ensemble from sklearn\nPipeline successfully instantiated\n", "name": "stdout"}, {"output_type": "stream", "text": "/opt/conda/envs/Python36/lib/python3.6/site-packages/sklearn/utils/deprecation.py:58: DeprecationWarning: Class Imputer is deprecated; Imputer was deprecated in version 0.20 and will be removed in 0.22. Import impute.SimpleImputer from sklearn instead.\n  warnings.warn(msg, category=DeprecationWarning)\n/opt/conda/envs/Python36/lib/python3.6/site-packages/sklearn/utils/deprecation.py:58: DeprecationWarning: Class Imputer is deprecated; Imputer was deprecated in version 0.20 and will be removed in 0.22. Import impute.SimpleImputer from sklearn instead.\n  warnings.warn(msg, category=DeprecationWarning)\n", "name": "stderr"}]}, {"metadata": {}, "cell_type": "markdown", "source": "### 3. Extract needed parameter values from AutoAI run metadata"}, {"metadata": {}, "cell_type": "code", "source": "\n# Metadata used in retrieving data and computing metrics.  Customize as necessary for your environment.\n#data_source='replace_with_path_and_csv_filename'\ntarget_label_name = _input_metadata['target_label_name']\nlearning_type = _input_metadata['learning_type']\noptimization_metric = _input_metadata['optimization_metric']\nrandom_state = _input_metadata['random_state']\ncv_num_folds = _input_metadata['cv_num_folds']\nholdout_fraction = _input_metadata['holdout_fraction']\nif 'data_provenance' in _input_metadata:\n    data_provenance = _input_metadata['data_provenance']\nelse:\n    data_provenance = None\nif 'pos_label' in _input_metadata and learning_type == 'classification':\n    pos_label = _input_metadata['pos_label']\nelse:\n    pos_label = None\n", "execution_count": 4, "outputs": []}, {"metadata": {}, "cell_type": "markdown", "source": "### 4. Create dataframe from dataset in IBM Cloud Object Storage or IBM Cloud Pak For Data"}, {"metadata": {}, "cell_type": "code", "source": "\n# @hidden_cell\n# The following code contains the credentials for a file in your IBM Cloud Object Storage.\n# You might want to remove those credentials before you share your notebook.\ncredentials_0 = {\n    'IAM_SERIVCE_ID': 'iam-ServiceId-db54b097-3670-4baf-8874-ec6263b2e21b',\n    'ENDPOINT': 'https://s3-api.us-geo.objectstorage.softlayer.net',\n    'IBM_AUTH_ENDPOINT': 'https://iam.bluemix.net/oidc/token/',\n    'IBM_API_KEY_ID': 'KD433tFGL_Rpkzw-OqsXXu67mknjRhG9EvIYpYMlSTda',\n    'BUCKET': 'predictingcovid19survivability-donotdelete-pr-lewxfxswowpg3k',\n    'FILE': 'data_asset/covid_19 dataset.csv_shaped_48a8694a.csv',\n    'SERVICE_NAME': 's3',\n    'ASSET_ID': '1',\n    }\n", "execution_count": 5, "outputs": []}, {"metadata": {}, "cell_type": "code", "source": "#  Read the data as a dataframe\nimport pandas as pd\n\ncsv_encodings=['UTF-8','Latin-1'] # supplement list of encodings as necessary for your data\ndf = None\nreadable = None  # if automatic detection fails, you can supply a filename here\n\n# First, obtain a readable object\n# IBM Cloud Object Storage data access\n# Assumes COS credentials are in a dictionary named 'credentials_0'\ncos_credentials = df = globals().get('credentials_0')       \nif readable is None and cos_credentials is not None:\n    print('accessing data via IBM Cloud Object Storage')\n    try:\n        import types\n        from botocore.client import Config\n        import ibm_boto3\n\n        def __iter__(self): return 0\n\n        if 'SERVICE_NAME' not in cos_credentials:  # in case of Studio-supplied credentials for a different dataset\n            cos_credentials['SERVICE_NAME'] = 's3'\n        client = ibm_boto3.client(service_name=cos_credentials['SERVICE_NAME'],\n            ibm_api_key_id=cos_credentials['IBM_API_KEY_ID'],\n            ibm_auth_endpoint=cos_credentials['IBM_AUTH_ENDPOINT'],\n            config=Config(signature_version='oauth'),\n            endpoint_url=cos_credentials['ENDPOINT'])\n\n        try:\n            readable = client.get_object(Bucket=cos_credentials['BUCKET'],Key=cos_credentials['FILE'])['Body']\n            # add missing __iter__ method, so pandas accepts readable as file-like object\n            if not hasattr(readable, \"__iter__\"): readable.__iter__ = types.MethodType( __iter__, readable )\n        except Exception as cos_access_exception:\n            print('unable to access data object in cloud object storage with credentials supplied')\n    except Exception as cos_exception:\n        print('unable to create client for cloud object storage')\n\n# IBM Cloud Pak for Data data access\nproject_filename = globals().get('project_filename')       \nif readable is None and 'credentials_0' in globals() and 'ASSET_ID' in credentials_0:\n    project_filename = credentials_0['ASSET_ID']\nif project_filename is not None:\n    print('attempting project_lib access to ' + str(project_filename))\n    try:\n        from project_lib import Project\n        project = Project.access()\n        storage_credentials = project.get_storage_metadata()\n        readable = project.get_file(project_filename)\n    except Exception as project_exception:\n        print('unable to access data using the project_lib interface and filename supplied')\n\n# Use data_provenance as filename if other access mechanisms are unsuccessful\nif readable is None and type(data_provenance) is str:\n    print('attempting to access local file using path and name ' + data_provenance)\n    readable = data_provenance\n\n# Second, use pd.read_csv to read object, iterating over list of csv_encodings until successful\nif readable is not None:\n    for encoding in csv_encodings:\n        try:\n            df = pd.read_csv(readable, encoding=encoding)\n            print('successfully loaded dataframe using encoding = ' + str(encoding))\n            break\n        except Exception as exception_csv:\n            print('unable to read csv using encoding ' + str(encoding))\n            print('handled error was ' + str(exception_csv))\n    if df is None:\n        print('unable to read file/object as a dataframe using supplied csv_encodings ' + str(csv_encodings))\n        print(\"Please use 'insert to code' on data panel to load dataframe.\")\n        raise(ValueError('unable to read file/object as a dataframe using supplied csv_encodings ' + str(csv_encodings)))\n\nif df is None:\n    print('Unable to access bucket/file in IBM Cloud Object Storage or asset in IBM Cloud Pak for Data with the parameters supplied.')\n    print('This is abnormal, but proceeding assuming the notebook user will supply a dataframe by other means.')\n    print(\"Please use 'insert to code' on data panel to load dataframe.\")\n\n", "execution_count": 6, "outputs": [{"output_type": "stream", "text": "accessing data via IBM Cloud Object Storage\nsuccessfully loaded dataframe using encoding = UTF-8\n", "name": "stdout"}]}, {"metadata": {}, "cell_type": "markdown", "source": "### 5. Preprocess Data"}, {"metadata": {}, "cell_type": "code", "source": "# Drop rows whose target is not defined\ntarget = target_label_name # your target name here\nif learning_type == 'regression':\n    df[target] = pd.to_numeric(df[target], errors='coerce')\ndf.dropna('rows', how='any', subset=[target], inplace=True)\n", "execution_count": 7, "outputs": []}, {"metadata": {}, "cell_type": "code", "source": "# extract X and y\ndf_X = df.drop(columns=[target])\ndf_y = df[target]\n", "execution_count": 8, "outputs": []}, {"metadata": {}, "cell_type": "code", "source": "# Detach preprocessing pipeline (which needs to see all training data)\npreprocessor_index = -1\npreprocessing_steps = [] \nfor i, step in enumerate(pipeline.steps):\n    preprocessing_steps.append(step)\n    if step[0]=='preprocessor':\n        preprocessor_index = i\n        break\nif len(pipeline.steps) > preprocessor_index+1 and pipeline.steps[preprocessor_index + 1][0] == 'cognito':\n    preprocessor_index += 1\n    preprocessing_steps.append(pipeline.steps[preprocessor_index])\nif preprocessor_index >= 0:\n    preprocessing_pipeline = Pipeline(memory=pipeline.memory, steps=preprocessing_steps)\n    pipeline = Pipeline(steps=pipeline.steps[preprocessor_index+1:])", "execution_count": 9, "outputs": []}, {"metadata": {}, "cell_type": "code", "source": "# Preprocess X\n# preprocessor should see all data for cross_validate on the remaining steps to match autoai scores\nknown_values_list.clear()  #  known_values_list is filled in by the preprocessing_pipeline if needed\npreprocessing_pipeline.fit(df_X.values, df_y.values)\nX_prep = preprocessing_pipeline.transform(df_X.values)", "execution_count": 10, "outputs": []}, {"metadata": {}, "cell_type": "markdown", "source": "### 6. Split data into Training and Holdout sets"}, {"metadata": {}, "cell_type": "code", "source": "# determine learning_type and perform holdout split (stratify conditionally)\nif learning_type is None:\n    # When the problem type is not available in the metadata, use the sklearn type_of_target to determine whether to stratify the holdout split\n    # Caution:  This can mis-classify regression targets that can be expressed as integers as multiclass, in which case manually override the learning_type\n    from sklearn.utils.multiclass import type_of_target\n    if type_of_target(df_y.values) in ['multiclass', 'binary']:\n        learning_type = 'classification'\n    else:\n        learning_type = 'regression'\n    print('learning_type determined by type_of_target as:',learning_type)\nelse:\n    print('learning_type specified as:',learning_type)\n    \nfrom sklearn.model_selection import train_test_split\nif learning_type == 'classification':\n    X, X_holdout, y, y_holdout = train_test_split(X_prep, df_y.values, test_size=holdout_fraction, random_state=random_state, stratify=df_y.values)\nelse:\n    X, X_holdout, y, y_holdout = train_test_split(X_prep, df_y.values, test_size=holdout_fraction, random_state=random_state)\n", "execution_count": 11, "outputs": [{"output_type": "stream", "text": "learning_type specified as: classification\n", "name": "stdout"}]}, {"metadata": {}, "cell_type": "markdown", "source": "### 7. Additional setup: Define a function that returns a scorer for the target's positive label"}, {"metadata": {}, "cell_type": "code", "source": "# create a function to produce a scorer for a given positive label\ndef make_pos_label_scorer(scorer, pos_label):\n    kwargs = {'pos_label':pos_label}\n    for prop in ['needs_proba', 'needs_threshold']:\n        if prop+'=True' in scorer._factory_args():\n            kwargs[prop] = True\n    if scorer._sign == -1:\n        kwargs['greater_is_better'] = False\n    from sklearn.metrics import make_scorer\n    scorer=make_scorer(scorer._score_func, **kwargs)\n    return scorer", "execution_count": 12, "outputs": []}, {"metadata": {}, "cell_type": "markdown", "source": "### 8. Fit pipeline, predict on Holdout set, calculate score, perform cross-validation"}, {"metadata": {}, "cell_type": "code", "source": "# fit the remainder of the pipeline on the training data\npipeline.fit(X,y)", "execution_count": 13, "outputs": [{"output_type": "execute_result", "execution_count": 13, "data": {"text/plain": "Pipeline(memory=None,\n     steps=[('estimator', GradientBoostingClassifier(criterion='friedman_mse', init=None,\n              learning_rate=0.1, loss='deviance', max_depth=3,\n              max_features=None, max_leaf_nodes=None,\n              min_impurity_decrease=0.0, min_impurity_split=None,\n              min_samples_leaf=1...    subsample=1.0, tol=0.0001, validation_fraction=0.1,\n              verbose=0, warm_start=False))])"}, "metadata": {}}]}, {"metadata": {}, "cell_type": "code", "source": "# predict on the holdout data\ny_pred = pipeline.predict(X_holdout)", "execution_count": null, "outputs": []}, {"metadata": {}, "cell_type": "code", "source": "# compute score for the optimization metric\n# scorer may need pos_label, but not all scorers take pos_label parameter\nfrom sklearn.metrics import get_scorer\nscorer = get_scorer(optimization_metric)\nscore = None\n#score = scorer(pipeline, X_holdout, y_holdout)  # this would suffice for simple cases\npos_label = None  # if you want to supply the pos_label, specify it here\nif pos_label is None and 'pos_label' in _input_metadata:\n    pos_label=_input_metadata['pos_label']\ntry:\n    score = scorer(pipeline, X_holdout, y_holdout)\nexcept Exception as e1:\n    if pos_label is None or str(pos_label)=='':\n        print('You may have to provide a value for pos_label in order for a score to be calculated.')\n        raise(e1)\n    else:\n        exception_string=str(e1)\n        if 'pos_label' in exception_string:\n            try:\n                scorer = make_pos_label_scorer(scorer, pos_label=pos_label)\n                score = scorer(pipeline, X_holdout, y_holdout)\n                print('Retry was successful with pos_label supplied to scorer')\n            except Exception as e2:\n                print('Initial attempt to use scorer failed.  Exception was:')\n                print(e1)\n                print('')\n                print('Retry with pos_label failed.  Exception was:')\n                print(e2)\n        else:\n            raise(e1)\n\nif score is not None:\n    print(score)", "execution_count": null, "outputs": []}, {"metadata": {}, "cell_type": "code", "source": "# cross_validate pipeline using training data\nfrom sklearn.model_selection import cross_validate\nfrom sklearn.model_selection import StratifiedKFold, KFold\nif learning_type == 'classification':\n    fold_generator = StratifiedKFold(n_splits=cv_num_folds, random_state=random_state)\nelse:\n    fold_generator = KFold(n_splits=cv_num_folds, random_state=random_state)\ncv_results = cross_validate(pipeline, X, y, cv=fold_generator, scoring={optimization_metric:scorer}, return_train_score=True)\nimport numpy as np\nnp.mean(cv_results['test_' + optimization_metric])", "execution_count": null, "outputs": []}, {"metadata": {}, "cell_type": "code", "source": "cv_results", "execution_count": null, "outputs": []}], "metadata": {"kernelspec": {"name": "python3", "display_name": "Python 3.6", "language": "python"}, "language_info": {"name": "python", "version": "3.6.9", "mimetype": "text/x-python", "codemirror_mode": {"name": "ipython", "version": 3}, "pygments_lexer": "ipython3", "nbconvert_exporter": "python", "file_extension": ".py"}}, "nbformat": 4, "nbformat_minor": 1}